llm:
  model_path: "model/llama_32_3B_Q4_K_M.gguf" # Path to the local LLM
  temperature: 0.3
  device: "cpu" # "cpu", "cuda", or "auto"
  max_tokens: 3000

generation:
  n_valid_values: 9
  n_invalid_values: 1
